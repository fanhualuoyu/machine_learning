{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.Parameter():Tensor的一种，常被用于模块参数(module parameter)。\n",
    "# Parameters 是 Variable 的子类。Paramenters和Modules一起使用的时候会有一些特殊的属性，\n",
    "# 即：当Paramenters赋值给Module的属性的时候，他会自动的被加到 Module的 参数列表中(即：会出现在 parameters() 迭代器中)。\n",
    "# 将Varibale赋值给Module属性则不会有这样的影响。\n",
    "# Variable 与 Parameter的另一个不同之处在于，Parameter不能被 volatile(即：无法设置volatile=True)而且默认requires_grad=True。Variable默认requires_grad=False。\n",
    "# 参数说明：\n",
    "#     data (Tensor) – parameter tensor.\n",
    "#     requires_grad (bool, optional) – 默认为True，在BP的过程中会对其求微分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.Module：所有神经网络模型的基类。\n",
    "# Modules也可以包含其它Modules,允许使用树结构嵌入他们。你可以将子模块赋值给模型属性。\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5) # 当前的nn.Conv2d模块就被赋值成为Model模块的一个子模块，成为“树结构”的叶子\n",
    "        self.conv2 = nn.Conv2d(20,20,5)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "# 通过上面方式赋值的submodule会被注册。当调用 .cuda() 的时候，submodule的参数也会转换为cuda Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_module(name, module)：将一个 child module 添加到当前 modle。被添加的module可以通过 name属性来获取。\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.add_module('conv',nn.Conv2d(10,20,4))\n",
    "        # self.conv = nn.Conv2d(10,20,4) 和上面这个增加module的方式等价\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# children():返回当前模型 子模块的迭代器。\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.add_module('conv',nn.Conv2d(10,20,4))\n",
    "        self.add_module('conv2',nn.Conv2d(20,10,4))\n",
    "model = Model()\n",
    "for sub_module in model.children():\n",
    "    print(sub_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv1): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      ")\n",
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# modules():返回一个包含 当前模型 所有模块的迭代器。\n",
    "#  重复的模块只被返回一次(children()也是)\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.add_module(\"conv\", nn.Conv2d(10, 20, 4))\n",
    "        self.add_module(\"conv1\", nn.Conv2d(20 ,10, 4))\n",
    "model = Model()\n",
    "for module in model.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# named_children():返回 包含 模型当前子模块 的迭代器，yield 模块名字和模块本身。\n",
    "for name,module in model.named_children():\n",
    "    if name in ['conv','conv1']:\n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential(* args)：一个时序容器。Modules 会以他们传入的顺序被添加到容器中。当然，也可以传入一个OrderedDict。\n",
    "from collections import OrderedDict\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1,20,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(20,64,5),\n",
    "    nn.ReLU()\n",
    ")\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1',nn.Conv2d(1,20,5)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('conv2',nn.Conv2d(20,64,5)),\n",
    "    ('relu2',nn.ReLU())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.ModuleList(modules=None)[source]:将submodules保存在一个list中。\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule,self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(10)])\n",
    "    def forward(self,x):\n",
    "        for i,l in enumerate(self.linears):\n",
    "            x = self.linears[i//2](x) + l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (10): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (10): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# append(module)[source]:等价于 list 的 append()\n",
    "# extend(modules)[source]:等价于 list 的 extend() 方法\n",
    "model = MyModule()\n",
    "model.linears.append(nn.Conv2d(10,20,5))\n",
    "for m in model.modules():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 34])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)：对输入数据的最后一维进行卷积\n",
    "# in_channels:输入信号的通道，可以理解为特征的维度\n",
    "# out_channels:卷积产生的通道，可以理解为卷积核的数量\n",
    "# kernel_size：卷积核的大小(只需要指定卷积核方向的大小)\n",
    "# 一维卷积层，输入的尺度是(N, C_in,L)，输出尺度（ N,C_out,L_out）.\n",
    "# 卷积的方向是一维的\n",
    "import torch\n",
    "conv1 = nn.Conv1d(256,100,2)\n",
    "input = torch.randn(32,35,256)\n",
    "input = input.permute(0,2,1) # 对最后一维进行卷积，设置卷积的方向正确\n",
    "output = conv1(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "# in_channels:输入信号的通道\n",
    "# out_channels:卷积产生的通道\n",
    "# kernel_size：卷积核的尺寸\n",
    "conv2 = nn.Conv2d(1,6,5)\n",
    "# 例子：一个样本：32*32*1，卷积核：5*5*6，输出：28*28*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大池化\n",
    "# torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):对于输入信号的输入通道，提供1维最大池化（max pooling）操作\n",
    "m = nn.MaxPool1d(3,stride=2) # 卷积核大小1*3\n",
    "input = Variable(torch.randn(2,4,5))\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7433,  2.3807,  0.8864, -0.7166, -0.0294],\n",
       "         [-0.1357,  1.4351, -0.8358, -1.5840,  0.4850],\n",
       "         [-1.0967, -0.3925, -2.4879, -0.2527,  0.9031],\n",
       "         [ 1.4497,  0.3739, -1.9373, -0.0063,  1.7655]],\n",
       "\n",
       "        [[ 0.6204,  0.6082, -0.1046, -1.1770,  0.8795],\n",
       "         [ 0.0079,  0.0912, -0.5472,  1.0131, -0.0741],\n",
       "         [-0.0253, -1.2980, -0.0243, -0.6525, -0.2064],\n",
       "         [ 0.9138,  0.1428,  0.8758,  1.9715,  0.5513]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3807,  0.8864],\n",
       "         [ 1.4351,  0.4850],\n",
       "         [-0.3925,  0.9031],\n",
       "         [ 1.4497,  1.7655]],\n",
       "\n",
       "        [[ 0.6204,  0.8795],\n",
       "         [ 0.0912,  1.0131],\n",
       "         [-0.0243, -0.0243],\n",
       "         [ 0.9138,  1.9715]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):对于输入信号的输入通道，提供2维最大池化（max pooling）操作\n",
    "m = nn.MaxPool2d(3,stride=2) # 卷积核大小3*3\n",
    "input = Variable(torch.randn(2,4,5))\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1703,  0.4636,  0.5321, -0.0598,  0.0120],\n",
       "         [ 1.5189, -0.5378,  0.5323,  0.2509,  0.2427],\n",
       "         [ 0.2651,  0.6894,  0.2843,  0.7283,  0.1857],\n",
       "         [ 0.6317, -1.3479, -0.0861, -2.5562,  0.2742]],\n",
       "\n",
       "        [[-0.9250,  0.4644,  0.6305,  1.2873,  0.0584],\n",
       "         [-0.2989,  1.0590, -2.6585,  0.1796, -0.8503],\n",
       "         [-0.0516,  2.2711,  0.2487,  1.1068,  1.6507],\n",
       "         [ 1.8002, -0.0152, -1.5697,  1.2718,  0.8300]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5189, 0.7283]],\n",
       "\n",
       "        [[2.2711, 1.6507]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output # 向下移动时超过了边界，所以直接计算下边的了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 5.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均池化\n",
    "# torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)：对信号的输入通道，提供1维平均池化\n",
    "m = nn.AvgPool1d(3,stride=3)\n",
    "m(Variable(torch.Tensor([[[1,2,3,4,5,6,7]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True):对信号的输入通道，提供2维的平均池化\n",
    "m = nn.AvgPool2d(3, stride=2)\n",
    "m = nn.AvgPool2d((3, 2), stride=(2, 1))\n",
    "input = Variable(torch.randn(20, 16, 50, 32))\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 50, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 24, 31])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "# torch.nn.ReLU(inplace=False):max(0, x)\n",
    "m = nn.ReLU()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.ReLU6(inplace=False): min(max(0,x), 6)\n",
    "m = nn.ReLU6()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.ELU(alpha=1.0, inplace=False):max(0,x) + min(0, alpha * (e^x - 1))\n",
    "m = nn.ELU()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.PReLU(num_parameters=1, init=0.25):max(0,x) + a * min(0,x)\n",
    "m = nn.PReLU()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.LeakyReLU(negative_slope=0.01, inplace=False):max(0, x) + {negative_slope} * min(0, x)\n",
    "m = nn.LeakyReLU(0.1)\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阈值\n",
    "# torch.nn.Threshold(threshold, value, inplace=False)：y=x,if x>=threshold y=value,if x<threshold\n",
    "# 小于threshold的会被value值代替\n",
    "m = nn.Threshold(0.1,20)\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Hardtanh(min_value=-1, max_value=1, inplace=False) ：f(x)=+1,if x>1; f(x)=−1,if x<−1; f(x)=x,otherwise\n",
    "m = nn.Hardtanh()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sigmoid:f(x)=1/(1+e−x)\n",
    "m = nn.Sigmoid()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6856, -0.4508])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn.Tanh:f(x)=(e^x−e^−x)/(e^x+e^x)\n",
    "m = nn.Tanh()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0201, -0.3699])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn.LogSigmoid：LogSigmoid(x) = log( 1 / ( 1 + e^{-x}))\n",
    "m = nn.LogSigmoid()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3738, 0.3428])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn.Softplus(beta=1, threshold=20)：f(x)=1/beta∗log(1+e^(beta∗xi))\n",
    "m = nn.Softplus()\n",
    "input = Variable(torch.randn(2))\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化，归一化\n",
    "# torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True)：对小批量(mini-batch)的2d或3d输入进行批标准化(Batch Normalization)操作\n",
    "# num_features： 来自期望输入的特征数\n",
    "# eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。\n",
    "# momentum： 动态均值和动态方差所使用的动量。默认为0.1。\n",
    "# affine： 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。\n",
    "m = nn.BatchNorm1d(100)\n",
    "input = Variable(torch.randn(20,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3313,  0.8838,  0.4155,  ..., -1.0946,  0.2698,  0.0939],\n",
       "        [ 0.5137, -0.7278, -0.2839,  ...,  0.5692, -1.3280,  0.9881],\n",
       "        [-0.5564,  0.1462,  0.9145,  ..., -1.2861,  0.4917, -2.0512],\n",
       "        ...,\n",
       "        [ 1.0394, -0.6032,  0.6419,  ..., -1.2895, -0.3141, -0.0785],\n",
       "        [-0.7362, -0.4512, -0.6516,  ...,  0.4143, -0.3473,  0.8825],\n",
       "        [-0.1977, -1.0536,  0.6910,  ...,  0.9317, -0.0268,  0.9081]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.BatchNorm1d(100,affine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3313,  0.8838,  0.4155,  ..., -1.0946,  0.2698,  0.0939],\n",
       "        [ 0.5137, -0.7278, -0.2839,  ...,  0.5692, -1.3280,  0.9881],\n",
       "        [-0.5564,  0.1462,  0.9145,  ..., -1.2861,  0.4917, -2.0512],\n",
       "        ...,\n",
       "        [ 1.0394, -0.6032,  0.6419,  ..., -1.2895, -0.3141, -0.0785],\n",
       "        [-0.7362, -0.4512, -0.6516,  ...,  0.4143, -0.3473,  0.8825],\n",
       "        [-0.1977, -1.0536,  0.6910,  ...,  0.9317, -0.0268,  0.9081]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True):对小批量(mini-batch)3d数据组成的4d输入进行批标准化(Batch Normalization)操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True):对小批量(mini-batch)4d数据组成的5d输入进行批标准化(Batch Normalization)操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环神经网络RNN\n",
    "# torch.nn.RNN( args, * kwargs)：将一个多层的 Elman RNN，激活函数为tanh或者ReLU，用于输入序列。\n",
    "# input_size – 输入x的特征数量。\n",
    "# hidden_size – 隐层的特征数量。\n",
    "# num_layers – RNN的层数。\n",
    "# nonlinearity – 指定非线性函数使用tanh还是relu。默认是tanh。\n",
    "# bias – 如果是False，那么RNN层就不会使用偏置权重 $b_ih$和$b_hh$,默认是True\n",
    "# batch_first – 如果True的话，那么输入Tensor的shape应该是[batch_size, time_step, feature],输出也是这样。\n",
    "# dropout – 如果值非零，那么除了最后一层外，其它层的输出都会套上一个dropout层。\n",
    "# bidirectional – 如果True，将会变成一个双向RNN，默认为False。\n",
    "rnn = nn.RNN(10,20,2)\n",
    "input = Variable(torch.randn(5,3,10))\n",
    "h0 = Variable(torch.randn(2,3,20)) # 初始隐藏层状态\n",
    "output,hn = rnn(input,h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 20])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.LSTM( args, * kwargs):将一个多层的 (LSTM) 应用到输入序列。\n",
    "lstm = nn.LSTM(10,20,2)\n",
    "input = Variable(torch.randn(5,3,10))\n",
    "h0 = Variable(torch.randn(2,3,20)) # 初始化隐状态\n",
    "c0 = Variable(torch.randn(2,3,20)) # 初始化细胞状态\n",
    "output,hn = lstm(input,(h0,c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.GRU( args, * kwargs)：将一个多层的GRU用于输入序列。\n",
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = Variable(torch.randn(5, 3, 10))\n",
    "h0 = Variable(torch.randn(2, 3, 20))\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性层\n",
    "# torch.nn.Linear(in_features, out_features, bias=True):对输入数据做线性变换：y=Ax+b\n",
    "m = nn.Linear(20,30)\n",
    "input = Variable(torch.randn(128,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layers\n",
    "# torch.nn.Dropout(p=0.5, inplace=False):随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。\n",
    "m = nn.Dropout(p=0.2) # p - 将元素置0的概率。默认值：0.5\n",
    "input = Variable(torch.randn(20,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Dropout2d(p=0.5, inplace=False):随机将输入张量中整个通道设置为0。对于每次前向调用，被置0的通道都是随机的。\\\n",
    "m = nn.Dropout2d(p=0.2)\n",
    "input = Variable(torch.randn(20, 16, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 32, 32])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入层\n",
    "# torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)\n",
    "# 一个保存了固定字典和大小的简单查找表。\n",
    "# 这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。\n",
    "# num_embeddings (int) - 嵌入字典的大小\n",
    "# embedding_dim (int) - 每个嵌入向量的大小\n",
    "# padding_idx (int, optional) - 如果提供的话，输出遇到此下标时用零填充\n",
    "# max_norm (float, optional) - 如果提供的话，会重新归一化词嵌入，使它们的范数小于提供的值\n",
    "# norm_type (float, optional) - 对于max_norm选项计算p范数时的p\n",
    "# scale_grad_by_freq (boolean, optional) - 如果提供的话，会根据字典中单词频率缩放梯度\n",
    "embedding = nn.Embedding(10,3)\n",
    "input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9119, -1.5520,  1.0155],\n",
       "         [ 1.7580,  1.5577, -0.7925],\n",
       "         [-1.7817, -0.3019, -0.9422],\n",
       "         [-0.3173, -0.2585,  0.1100]],\n",
       "\n",
       "        [[-1.7817, -0.3019, -0.9422],\n",
       "         [ 1.8761, -1.1819, -0.5666],\n",
       "         [ 1.7580,  1.5577, -0.7925],\n",
       "         [-0.2947,  0.3488,  0.7625]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance functions\n",
    "# torch.nn.PairwiseDistance(p=2, eps=1e-06)\n",
    "# x (Tensor): 包含两个输入batch的张量\n",
    "# p (real): 范数次数，默认值：2\n",
    "pdist = nn.PairwiseDistance(2)\n",
    "input1 = Variable(torch.randn(100,128))\n",
    "input2 = Variable(torch.randn(100,128))\n",
    "output = pdist(input1,input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.8760, 15.8148, 14.5633, 16.8769, 16.4922, 16.4681, 14.4260, 15.1053,\n",
       "        16.4096, 17.1762, 16.5983, 16.1969, 14.8293, 15.0403, 15.7185, 16.9729,\n",
       "        17.0383, 17.1353, 15.5264, 14.4156, 15.0209, 15.5449, 15.9948, 14.1881,\n",
       "        15.5176, 14.6655, 17.8570, 15.7895, 16.8613, 17.0910, 16.7079, 17.1548,\n",
       "        15.6098, 15.6695, 17.0571, 15.3124, 16.0477, 15.3989, 15.9452, 16.1982,\n",
       "        16.6993, 15.8014, 17.1551, 18.4247, 15.3626, 16.8866, 17.2968, 14.4167,\n",
       "        16.5108, 14.7482, 16.0574, 16.9534, 15.9314, 15.7581, 15.1447, 16.3232,\n",
       "        15.6758, 16.4840, 15.0669, 16.0622, 16.3184, 15.1068, 16.3507, 17.1800,\n",
       "        16.0104, 16.5719, 16.9062, 15.8765, 14.4330, 16.0034, 16.4708, 14.1615,\n",
       "        16.3936, 16.1600, 15.9391, 16.1784, 15.0878, 15.6841, 15.8603, 16.3570,\n",
       "        14.8949, 17.0658, 13.2113, 16.3596, 14.9465, 15.8039, 17.2484, 15.5467,\n",
       "        14.9018, 18.0659, 15.4821, 13.0948, 14.9740, 18.2094, 16.7981, 16.7069,\n",
       "        15.8204, 15.1274, 16.1204, 16.6716])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1291)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss functions\n",
    "# torch.nn.L1Loss(size_average=True):创建一个衡量输入x(模型预测输出)和目标y之间差的绝对值的平均值的标准。\n",
    "# loss(x,y)=1/n*∑|xi−yi|\n",
    "L1Loss = nn.L1Loss()\n",
    "L1Loss(input1,input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss(size_average=True):创建一个衡量输入x(模型预测输出)和目标y之间均方误差标准。\n",
    "# loss(x,y)=1/n*∑(xi−yi)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch.nn.CrossEntropyLoss(weight=None, size_average=True):此标准将LogSoftMax和NLLLoss集成到一个类中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-714509db1f5d>:6: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = loss(m(input),target)\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.NLLLoss(weight=None, size_average=True):负的log likelihood loss损失。用于训练一个n类分类器。\n",
    "m = nn.LogSoftmax()\n",
    "loss = nn.NLLLoss()\n",
    "input = Variable(torch.randn(3,5),requires_grad=True)\n",
    "target = Variable(torch.LongTensor([1,0,4]))\n",
    "output = loss(m(input),target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
