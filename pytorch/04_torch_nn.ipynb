{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.Parameter():Tensor的一种，常被用于模块参数(module parameter)。\n",
    "# Parameters 是 Variable 的子类。Paramenters和Modules一起使用的时候会有一些特殊的属性，\n",
    "# 即：当Paramenters赋值给Module的属性的时候，他会自动的被加到 Module的 参数列表中(即：会出现在 parameters() 迭代器中)。\n",
    "# 将Varibale赋值给Module属性则不会有这样的影响。\n",
    "# Variable 与 Parameter的另一个不同之处在于，Parameter不能被 volatile(即：无法设置volatile=True)而且默认requires_grad=True。Variable默认requires_grad=False。\n",
    "# 参数说明：\n",
    "#     data (Tensor) – parameter tensor.\n",
    "#     requires_grad (bool, optional) – 默认为True，在BP的过程中会对其求微分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.Module：所有神经网络模型的基类。\n",
    "# Modules也可以包含其它Modules,允许使用树结构嵌入他们。你可以将子模块赋值给模型属性。\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5) # 当前的nn.Conv2d模块就被赋值成为Model模块的一个子模块，成为“树结构”的叶子\n",
    "        self.conv2 = nn.Conv2d(20,20,5)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))\n",
    "# 通过上面方式赋值的submodule会被注册。当调用 .cuda() 的时候，submodule的参数也会转换为cuda Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_module(name, module)：将一个 child module 添加到当前 modle。被添加的module可以通过 name属性来获取。\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.add_module('conv',nn.Conv2d(10,20,4))\n",
    "        # self.conv = nn.Conv2d(10,20,4) 和上面这个增加module的方式等价\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# children():返回当前模型 子模块的迭代器。\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.add_module('conv',nn.Conv2d(10,20,4))\n",
    "        self.add_module('conv2',nn.Conv2d(20,10,4))\n",
    "model = Model()\n",
    "for sub_module in model.children():\n",
    "    print(sub_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv1): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n",
      ")\n",
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# modules():返回一个包含 当前模型 所有模块的迭代器。\n",
    "#  重复的模块只被返回一次(children()也是)\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.add_module(\"conv\", nn.Conv2d(10, 20, 4))\n",
    "        self.add_module(\"conv1\", nn.Conv2d(20 ,10, 4))\n",
    "model = Model()\n",
    "for module in model.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))\n",
      "Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# named_children():返回 包含 模型当前子模块 的迭代器，yield 模块名字和模块本身。\n",
    "for name,module in model.named_children():\n",
    "    if name in ['conv','conv1']:\n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential(* args)：一个时序容器。Modules 会以他们传入的顺序被添加到容器中。当然，也可以传入一个OrderedDict。\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1,20,5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(20,64,5),\n",
    "    nn.ReLU()\n",
    ")\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1',nn.Conv2d(1,20,5)),\n",
    "    ('relu1',nn.ReLU()),\n",
    "    ('conv2',nn.Conv2d(20,64,5)),\n",
    "    ('relu2',nn.ReLU())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.ModuleList(modules=None)[source]:将submodules保存在一个list中。\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule,self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(10)])\n",
    "    def forward(self,x):\n",
    "        for i,l in enumerate(self.linears):\n",
    "            x = self.linears[i//2](x) + l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (10): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (5): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (6): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (7): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (8): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (9): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (10): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# append(module)[source]:等价于 list 的 append()\n",
    "# extend(modules)[source]:等价于 list 的 extend() 方法\n",
    "model = MyModule()\n",
    "model.linears.append(nn.Conv2d(10,20,5))\n",
    "for m in model.modules():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 34])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)：对输入数据的最后一维进行卷积\n",
    "# in_channels:输入信号的通道，可以理解为特征的维度\n",
    "# out_channels:卷积产生的通道，可以理解为卷积核的数量\n",
    "# kernel_size：卷积核的大小(只需要指定卷积核方向的大小)\n",
    "# 一维卷积层，输入的尺度是(N, C_in,L)，输出尺度（ N,C_out,L_out）.\n",
    "# 卷积的方向是一维的\n",
    "import torch\n",
    "conv1 = nn.Conv1d(256,100,2)\n",
    "input = torch.randn(32,35,256)\n",
    "input = input.permute(0,2,1) # 对最后一维进行卷积，设置卷积的方向正确\n",
    "output = conv1(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "# in_channels:输入信号的通道\n",
    "# out_channels:卷积产生的通道\n",
    "# kernel_size：卷积核的尺寸\n",
    "conv2 = nn.Conv2d(1,6,5)\n",
    "# 例子：一个样本：32*32*1，卷积核：5*5*6，输出：28*28*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
